{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26f0e652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://hadoop-master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://10.1.8.101:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa4091aeeb8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abde1032",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", \"AKIASIV2BBOBY7OLXVET\")\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", \"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ed842ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fa4091aee80>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "# Define schema of the csv\n",
    "d_patientsSchema = StructType() \\\n",
    "    .add(\"subject_id\", \"string\") \\\n",
    "    .add(\"sex\", \"string\") \\\n",
    "    .add(\"dob\", \"string\") \\\n",
    "    .add(\"dod\", \"string\") \\\n",
    "    .add(\"hospital_expire_flg\", \"string\")\n",
    "\n",
    "dfD_patients = spark.readStream.option(\"sep\", \",\").option(\"header\", \"true\").schema(d_patientsSchema).csv(\"s3a://sister-team/spark-streamming/d_patients\")\n",
    "\n",
    "# dfD_patients = spark.readStream.option(\"sep\", \",\").option(\"inferSchema\" , \"true\").option(\"header\", \"true\").csv(\"s3a://sister-team/spark-streamming/d_patients\")\n",
    "dfD_patients \\\n",
    "   .writeStream \\\n",
    "   .format('delta') \\\n",
    "   .outputMode(\"append\") \\\n",
    "   .option(\"checkpointLocation\", \"/tmp/d_patients/checkpointD_patients\") \\\n",
    "   .start(\"/tmp/d_patients\")\n",
    "\n",
    "# dfCSV.writeStream.format(\"console\").outputMode(\"append\").start().awaitTermination()\n",
    "# dfCSV.writeStream.format('delta').outputMode(\"append\").option('path',\"tmp/test-kafka\").option(\"checkpointLocation\", \"tmp/test-kafka/table2\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "100bf7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-------------------+-------------------+-------------------+\n",
      "|subject_id|sex|                dob|                dod|hospital_expire_flg|\n",
      "+----------+---+-------------------+-------------------+-------------------+\n",
      "|        31|  M| 17/5/2606 00:00:00| 29/8/2678 00:00:00|                  Y|\n",
      "|        37|  M| 11/9/3195 00:00:00|31/12/3265 00:00:00|                  N|\n",
      "|        56|  F| 26/5/2553 00:00:00| 23/1/2644 00:00:00|                  Y|\n",
      "|        61|  M|27/10/3297 00:00:00|  9/2/3353 00:00:00|                  Y|\n",
      "|        67|  M|  4/6/2903 00:00:00|29/11/2976 00:00:00|                  Y|\n",
      "|        78|  M|  8/8/2729 00:00:00| 11/3/2781 00:00:00|                  N|\n",
      "|         3|  M| 28/2/2606 00:00:00|  2/5/2683 00:00:00|                  N|\n",
      "|        12|  M| 14/5/2803 00:00:00| 9/10/2875 00:00:00|                  Y|\n",
      "|        21|  M| 22/5/3051 00:00:00| 28/3/3139 00:00:00|                  Y|\n",
      "|        26|  M|  2/3/3007 00:00:00|22/12/3080 00:00:00|                  N|\n",
      "+----------+---+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(\"/tmp/d_patients\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5341c2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fa408f6e438>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admissionsSchema = StructType() \\\n",
    "    .add(\"hadm_id\", \"string\") \\\n",
    "    .add(\"subject_id\", \"string\") \\\n",
    "    .add(\"admit_dt\", \"string\") \\\n",
    "    .add(\"disch_dt\", \"string\")\n",
    "\n",
    "dfAdmissions = spark.readStream.option(\"sep\", \",\").option(\"header\", \"true\").schema(admissionsSchema).csv(\"s3a://sister-team/spark-streamming/admissions\")\n",
    "dfAdmissions \\\n",
    "   .writeStream \\\n",
    "   .format('delta') \\\n",
    "   .outputMode(\"append\") \\\n",
    "   .option(\"checkpointLocation\", \"/tmp/admissions/checkpointAdmissions\") \\\n",
    "   .start(\"/tmp/admissions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64009ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------------+-------------------+\n",
      "|hadm_id|subject_id|           admit_dt|           disch_dt|\n",
      "+-------+----------+-------------------+-------------------+\n",
      "|     12|      8652| 11/9/3125 00:00:00| 22/9/3125 00:00:00|\n",
      "|     15|      7728|  8/4/3491 00:00:00| 16/4/3491 00:00:00|\n",
      "|     34|     17025|14/10/3118 00:00:00|25/10/3118 00:00:00|\n",
      "|     35|     15784| 2/10/3036 00:00:00| 7/10/3036 00:00:00|\n",
      "|     45|     23014|  1/5/3108 00:00:00|  5/6/3108 00:00:00|\n",
      "|     47|      9129| 19/6/3129 00:00:00| 25/6/3129 00:00:00|\n",
      "|      2|     24807|  8/7/3033 00:00:00| 17/7/3033 00:00:00|\n",
      "|      3|      7675| 16/5/3388 00:00:00| 21/5/3388 00:00:00|\n",
      "|      6|     23547|  3/4/3381 00:00:00| 22/4/3381 00:00:00|\n",
      "|     10|     14884| 28/8/3015 00:00:00|  5/9/3015 00:00:00|\n",
      "+-------+----------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(\"/tmp/admissions\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfceb45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
